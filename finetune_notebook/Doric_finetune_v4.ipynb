{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT6mw6Be0tWq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV58Cy7H1LXI"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfxUvb4t1UH2"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    target_modules=['q_proj','k_proj','v_proj','o_proj']\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1gsWukR1XR6"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KiF9VR81b1F"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW3BeqRp1d3N"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"franco334578/doric-conversations\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYQweFdS2AGY"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_data_formats\n",
        "dataset = standardize_data_formats(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_4DNylj2BeJ"
      },
      "outputs": [],
      "source": [
        "dataset[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LU4gBqT2FY7"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk-jH5-Y2Hbo"
      },
      "outputs": [],
      "source": [
        "dataset[100][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZAdYkYEvbg9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ['WANDB_API_KEY'] = userdata.get(\"WANDB_API_KEY\")\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP7PRtQq2Lut"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_name_with_time = f\"doric_v4_{current_time}\"\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size=4, # NEW\n",
        "        gradient_accumulation_steps = 8, #NEW\n",
        "        warmup_steps=0,\n",
        "        warmup_ratio = 0.075,\n",
        "        num_train_epochs = 3,\n",
        "        # max_steps = 30,\n",
        "        learning_rate = 5e-4, #NEW changed from 2e-4\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"wandb\",\n",
        "        run_name=run_name_with_time\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbFPfWxu2Ted"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWnZZS8V2aOv"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPz02-1i7M-0"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\" : \"text\",\n",
        "        \"text\" : \"What existed before the big bang?\",\n",
        "    }]\n",
        "}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        ")\n",
        "outputs = model.generate(\n",
        "    **inputs.to(\"cuda\"),\n",
        "    max_new_tokens = 2048, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga9p9PiG7dBV"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"gemma-3-doric\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3-doric\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCWYtGmY7qQ-"
      },
      "outputs": [],
      "source": [
        "# if False:\n",
        "#     from unsloth import FastModel\n",
        "#     model, tokenizer = FastModel.from_pretrained(\n",
        "#         model_name = \"gemma-3-doric\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "#         max_seq_length = 2048,\n",
        "#         load_in_4bit = True,\n",
        "#     )\n",
        "\n",
        "# messages = [{\n",
        "#     \"role\": \"user\",\n",
        "#     \"content\": [{\"type\" : \"text\", \"text\" : \"Explain game theory using Python\",}]\n",
        "# }]\n",
        "# inputs = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     add_generation_prompt = True, # Must add for generation\n",
        "#     tokenize = True,\n",
        "#     return_tensors = \"pt\",\n",
        "#     return_dict = True,\n",
        "# )\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# _ = model.generate(\n",
        "#     **inputs.to(\"cuda\"),\n",
        "#     max_new_tokens = 1440, # Increase for longer outputs!\n",
        "#     # Recommended Gemma-3 settings!\n",
        "#     temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "#     streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "62mTQCxy76ye"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Always keep this\n",
        "tokenizer.save_pretrained(\"gemma-3-doric\")\n",
        "\n",
        "if False: # Change to True to save/upload GGUF\n",
        "    from unsloth import FastModel\n",
        "    model.save_pretrained(\"gemma-3-doric\")  # Local saving\n",
        "    model.push_to_hub_gguf(\n",
        "        \"franco334578/unsloth-gemma-3-4b-it-doric-v4-GGUF\",\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\",], # Only Q8_0, BF16, F16 supported\n",
        "        token = userdata.get(\"HF_TOKEN\"),\n",
        "    )\n",
        "\n",
        "if True: # Change to True to save/upload float16 for VLLM\n",
        "  model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)\n",
        "  model.push_to_hub_merged(\"franco334578/unsloth-gemma-3-4b-it-doric-v4-f16\", tokenizer, save_method = \"merged_16bit\", token = userdata.get(\"HF_TOKEN\"),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NCszw1p595J-"
      },
      "outputs": [],
      "source": [
        "# # run in a Colab shell cell\n",
        "# !apt-get update\n",
        "# !apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\n",
        "# !git clone https://github.com/ggerganov/llama.cpp\n",
        "# !cmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\n",
        "# !cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli\n",
        "# !cp llama.cpp/build/bin/llama-* llama.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "decMegiG96X2"
      },
      "source": [
        "## Run local model in LLama.cppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VcurDZ6-IuL"
      },
      "outputs": [],
      "source": [
        "# alternatively open a a Terminal and run\n",
        "\n",
        "# ./llama.cpp/llama-cli \\\n",
        "#   -m gemma-3-4b-it.Q8_0.gguf \\\n",
        "#   --jinja \\\n",
        "#   --threads -1 \\\n",
        "#   --ctx-size 40960 \\\n",
        "#   --n-gpu-layers 99 \\\n",
        "#   --temp 0.7 \\\n",
        "#   --top-p 0.95\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo79RoNtFgCz"
      },
      "outputs": [],
      "source": [
        "# # or load from HF\n",
        "\n",
        "# !./llama.cpp/llama-cli -hf franco334578/unsloth-gemma-3-4b-it-doric-v4-GGUF"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}